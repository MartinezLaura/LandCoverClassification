{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classification of LC from Semnatically segmented pictures\n",
    "\n",
    "With a sample of 1120 photos covering eight major LC types over the European Union. \n",
    "\n",
    "We applied semantic segmentation on these photos using a neural network trained with the ADE20k dataset. \n",
    "\n",
    "For each photo, we extracted the original LC identified, the segmented classes, and the pixel count for each class.\n",
    "\n",
    "Using the latter as input features, we trained a Random Forest to classify the LC.\n",
    "\n",
    "The results show a mean F1-score of 89\\%, increasing to 93\\% when the Wetland class is not considered. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "\n",
    "import os, glob\n",
    "import sys\n",
    "sys.path.append(\"./git\")\n",
    "import RF_utils as utils\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, make_scorer, precision_recall_fscore_support\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import tree\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the paths.\n",
    "\n",
    "\n",
    "gt_path is the path to a csv file containing all the pixel count per class of the images used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/eos/jeodpp/data/projects/REFOCUS/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_to_resnets = '{}/LandCOverCLass/outputs/'.format(project_path)\n",
    "gt_path = '{}/LandCOverCLass/inputs/Final-dataset-140xclass.csv'.format(project_path)\n",
    "out_path = '{}/LandCOverCLass/outputs/'.format(project_path)\n",
    "path_images = '{}/data/LandCOverCLass/inputs/dataset/'.format(project_path)\n",
    "path_plots = '{}/LandCOverCLass/Plots/'.format(project_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the class label in the cvs\n",
    "class_gt = 'lc1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "data = pd.read_csv(gt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set in another place:\n",
    "\n",
    "THRESH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial histogram of the ditribution of the dataset by LC class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {'A':\"Artificial Land\", 'B':\"Cropland\", 'C':\"Woodland\", 'D':\"Shrubland\",\n",
    "                       'E':\"Grassland\", 'F':\"Bare soil\", 'G':\"Water areas\", 'H':\"Wetland\"}\n",
    "\n",
    "df = data['lc1'].value_counts()\n",
    "\n",
    "# Replace the letters with the labels\n",
    "df.index = df.index.map(names)\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = df.plot.bar(x='land_type', y='values')\n",
    "ax.set_xlabel('Land Type')\n",
    "ax.set_ylabel('Values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what's the best ResNet(trained with ADE20K) used for inferencing for the Random Forest classifier\n",
    "\n",
    "Since we use in inference several backbones of the Deeplav V3 + (see https://github.com/dmlc/gluon-cv)\n",
    "\n",
    "We run a tune parameters in the random forest also takinf the backbones as a parameter to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "files = glob.glob('{}test*.csv'.format(Path_to_resnets))\n",
    "\n",
    "#Create csv to save all data\n",
    "print(files)\n",
    "hypertune = pd.DataFrame(columns=['model', 'best-params', 'acc-train', 'acc-test','Precision', 'Recall', 'F_score'])\n",
    "gt = pd.read_csv(gt_path)\n",
    "THRESH = 0\n",
    "\n",
    "train, test = utils.split_dataset(gt, class_gt, 'Dataset_partition', out_path)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "          \n",
    "for f in files:\n",
    "    print('----------------{}--------------'.format(f))\n",
    "    results = pd.read_csv(f)\n",
    "    model = str(f).split('/')[-1]\n",
    "    results = utils.erase_empty_cols(results)\n",
    "    X_train, X_test, y_train, y_test = utils.prepare_set(train, test, results)\n",
    "    \n",
    "\n",
    "    # Random Forest\n",
    "    rf_param = { \n",
    "        'n_estimators': [50, 100, 150, 200, 300],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth' : [ 20, 50, 75, 100,125,150],\n",
    "        'min_samples_leaf': [1, 2, 4,8,10,12,14,16,18,20],\n",
    "        'min_samples_split' : [2, 5,8,10,13,15,18,20],\n",
    "        'criterion' :['gini', 'entropy'],\n",
    "        'bootstrap': [True, False],\n",
    "    }\n",
    "\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 50)]\n",
    "    max_depth.append(None)\n",
    "    rf_dist = {'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 2000, num = 50)],\n",
    "                 'max_features':  ['auto', 'sqrt', 'log2'],\n",
    "                 'max_depth': max_depth,\n",
    "                 'min_samples_split': [int(x) for x in np.linspace(start = 1, stop = 50, num = 50)],\n",
    "                 'min_samples_leaf': [int(x) for x in np.linspace(start = 1, stop = 50, num = 50)],\n",
    "                 'bootstrap': [True, False],\n",
    "                 'criterion' :['gini', 'entropy']}\n",
    "\n",
    "\n",
    "    rf_grid = utils.search_params(RandomForestClassifier(), rf_param, \"grid\", X_train, y_train, cv)\n",
    "    \n",
    "    clf = RandomForestClassifier(**rf_grid.best_params_).fit(X_train, y_train)\n",
    "    \n",
    "    scores = cross_validate(clf, X_test, y_test, cv=5, scoring=('accuracy','precision_macro', 'recall_macro','f1_macro'))\n",
    "    \n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores['test_accuracy'].mean(), scores['test_accuracy'].std()))\n",
    "    print(\"%0.2f precision_macro with a standard deviation of %0.2f\" % (scores['test_precision_macro'].mean(), scores['test_precision_macro'].std()))\n",
    "    print(\"%0.2f recall_macro with a standard deviation of %0.2f\" % (scores['test_recall_macro'].mean(), scores['test_recall_macro'].std()))\n",
    "    print(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores['test_f1_macro'].mean(), scores['test_f1_macro'].std()))\n",
    "\n",
    "\n",
    "    hypertune = hypertune.append({'model': model, 'best-params': rf_grid.best_params_, 'acc-train': rf_grid.best_score_, \\\n",
    "                                        'acc-test':scores['test_accuracy'].mean(), 'Precision': scores['test_precision_macro'].mean(), \\\n",
    "                                        'Recall': scores['test_recall_macro'].mean(), 'F_score': scores['test_f1_macro'].mean()}, ignore_index=True)\n",
    "\n",
    "    hypertune.to_csv('{}/hypertune.csv'.format(out_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which is the best model and the parameters selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypertune = pd.read_csv('{}/hypertune.csv'.format(out_path))\n",
    "model_sel = hypertune.sort_values(by=['acc-train', 'F_score'], ascending=False).head(1)\n",
    "\n",
    "print(model_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the backbone that works best\n",
    "print( model_sel['model'].item())\n",
    "\n",
    "final_path_results = '{}/{}'.format(Path_to_resnets, model_sel['model'].item())\n",
    "results = pd.read_csv(final_path_results)\n",
    "results = results.loc[:, ~results.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final metrics of the bests model With/Without Wetlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the parameters from the tunning proces, we run on the test set the acuracy final metrics,taking into account the Wetland class and also excluding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the backbone that works best\n",
    "\n",
    "print(model_sel['model'].item())\n",
    "print(model_sel['best-params'].item())\n",
    "\n",
    "final_path_results = '{}{}'.format(Path_to_resnets, model_sel['model'].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "gt = pd.read_csv(gt_path)\n",
    "results = pd.read_csv(final_path_results)\n",
    "\n",
    "with open('{}Dataset_partition.pickle'.format(out_path), \"rb\") as input_file:    \n",
    "    e = pickle.load(input_file)          \n",
    "train = e[0]\n",
    "test= e[1]\n",
    "\n",
    "\n",
    "hypertune = pd.read_csv('{}/hypertune.csv'.format(out_path))\n",
    "\n",
    "f = '{}{}'.format(Path_to_resnets, model_sel['model'].item())\n",
    "print('----------------{}--------------'.format(f))\n",
    "results = pd.read_csv(f)\n",
    "model = str(f).split('/')[-1]\n",
    "results = utils.erase_empty_cols(results)\n",
    "X_train, X_test, y_train, y_test = utils.prepare_set(train, test, results)\n",
    "\n",
    "\n",
    "params = ast.literal_eval(model_sel['best-params'].item())\n",
    "print(params)\n",
    "clf = RandomForestClassifier(**params)\n",
    "\n",
    "print('----------------ALL CLASSES FINAL PERFoRMANCE--------------')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "labels = unique_labels(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
    "\n",
    "cfmatrix = confusion_matrix(y_true=y_test, y_pred = y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfmatrix,\n",
    "                              display_labels=labels)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "disp.figure_.savefig('{}CM-Allclasses.jpg'.format(path_plots))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics without the Wetlands classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Filter class Wetlands\"\"\"\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "gt = pd.read_csv(gt_path)\n",
    "results = pd.read_csv(final_path_results)\n",
    "\n",
    "with open('{}Dataset_partition.pickle'.format(out_path), \"rb\") as input_file:    \n",
    "    e = pickle.load(input_file)\n",
    "          \n",
    "train = e[0]\n",
    "test= e[1]\n",
    "\n",
    "hypertune = pd.read_csv('{}/hypertune.csv'.format(out_path))\n",
    "\n",
    "f = '{}{}'.format(Path_to_resnets, model_sel['model'].item())\n",
    "print('----------------{}--------------'.format(f))\n",
    "results = pd.read_csv(f)\n",
    "model = str(f).split('/')[-1]\n",
    "results = utils.erase_empty_cols(results)\n",
    "X_train, X_test, y_train, y_test = utils.prepare_set(train, test, results, wet = True)\n",
    "\n",
    "params = ast.literal_eval(model_sel['best-params'].item())\n",
    "\n",
    "clf = RandomForestClassifier(**params)\n",
    "\n",
    "print('---------------- CLASSES WITHOUT WETLAND FINAL PERFoRMANCE--------------')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "labels = unique_labels(y_test, y_pred)\n",
    "\n",
    "with open('{}Predictions-noWetlands.pickle'.format(out_path), 'wb') as handle:\n",
    "    pickle.dump([y_pred, y_proba, labels, y_test], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='macro'))\n",
    "\n",
    "cfmatrix = confusion_matrix(y_true=y_test, y_pred = y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfmatrix,\n",
    "                              display_labels=labels)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "disp.figure_.savefig('{}CM-NoWetlands.jpg'.format(path_plots))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Plots of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show one of the trees in the Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (12,12), dpi=800)\n",
    "tree.plot_tree(clf.estimators_[10],\n",
    "               feature_names = results.columns, \n",
    "               class_names=labels,\n",
    "               filled = True);\n",
    "fig.savefig('{}rf_individualtree.png'.format(path_plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SanKey diagram showing the relation between the LUCAS LC Classes and the ADE20K classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(final_path_results)\n",
    "field = 'lc'\n",
    "\n",
    "aux = results.iloc[:,:-2]\n",
    "T = 1920000 * THRESH\n",
    "aux[aux < T] = 0\n",
    "aux = aux.where(aux == 0, 1)\n",
    "df_unary = pd.concat([aux, results.iloc[:,-2:]], axis = 1)\n",
    "\n",
    "#plot\n",
    "utils.plot_relation_feat_LC(df_unary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Semantic segmentation with Deeplabv3+ trained with the ADE20k datasetand a pie diagram showing the share of each class detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_class_plot(path_plots, results, path_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing the LUCAS point photo distribution by pixel area of the ADE20k objects segmented and the probability output by the Random Forest for each LC class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_falsep(path_images, f, proba, test, pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SVM- Training.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
